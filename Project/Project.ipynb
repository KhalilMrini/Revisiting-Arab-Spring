{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import codecs\n",
    "import zlib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import spinn3rApi_pb2 as proto_api\n",
    "import protoStream_pb2 as proto_stream\n",
    "from google.protobuf.internal.decoder import _DecodeVarint32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each .protostream file contains exactly 200 distinct records\n",
    "\n",
    "def read_protostream_file(file):\n",
    "    decoder = _DecodeVarint32\n",
    "    \n",
    "    ## get the three types of protobuf messages we expect to see\n",
    "    header    = proto_stream.ProtoStreamHeader()\n",
    "    delimiter = proto_stream.ProtoStreamDelimiter()\n",
    "    \n",
    "    ## get the header\n",
    "    # length, pos = decoder(file, pos) => pos is starting point, length is the length of the record\n",
    "    length, pos = decoder(file, 0)\n",
    "    header.ParseFromString(file[pos:pos + length])\n",
    "    # print(header)\n",
    "    ## should check its contents\n",
    "\n",
    "    pos += length\n",
    "    data_rows = []\n",
    "\n",
    "    while pos < len(file):\n",
    "        length, pos = decoder(file, pos)\n",
    "        delimiter.ParseFromString(file[pos:pos + length])\n",
    "        if delimiter.delimiter_type == delimiter.END:\n",
    "            break\n",
    "        elif delimiter.delimiter_type == delimiter.ENTRY:\n",
    "            pos += length\n",
    "            length, pos = decoder(file, pos)\n",
    "            entry = proto_api.Entry()\n",
    "            entry.ParseFromString(file[pos:pos + length])\n",
    "            data = {}\n",
    "            \n",
    "            # Language\n",
    "            try:\n",
    "                data['Lang_Code'] = entry.feed_entry.lang[0].code\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Language Probability\n",
    "            try:\n",
    "                data['Lang_Prob'] = entry.feed_entry.lang[0].probability\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Title\n",
    "            try:\n",
    "                data['Post_Title'] = entry.feed_entry.title\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Content\n",
    "            try:\n",
    "                data['Post_Content'] = BeautifulSoup(zlib.decompress(\n",
    "                    entry.feed_entry.content.data), \"html.parser\").text\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Link\n",
    "            try:\n",
    "                data['Post_Link'] = entry.feed_entry.link[0].href\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Author Name\n",
    "            try:\n",
    "                data['Author_Name'] = entry.feed_entry.author[0].name\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Author Link\n",
    "            try:\n",
    "                data['Author_Link'] = entry.feed_entry.author[0].link[0].href\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Date & Time\n",
    "            try:\n",
    "                data['Datetime'] = entry.feed_entry.last_published\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Identifier\n",
    "            try:\n",
    "                data['Identifier'] = entry.feed_entry.identifier\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Spam Probability\n",
    "            try:\n",
    "                data['Spam'] = entry.feed_entry.spam_probability\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Publisher Type\n",
    "            try:\n",
    "                data['Type'] = entry.source.publisher_type\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Category\n",
    "            try:\n",
    "                data['Category'] = entry.feed_entry.category\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            data_rows.append(data)\n",
    "        pos += length\n",
    "    return data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = '/Volumes/Khalil Mrini/'\n",
    "FILES = '01-14-OTHER 01-14-SOCIAL_MEDIA 02-11-OTHER 02-11-SOCIAL_MEDIA'.split(' ')\n",
    "EXTENSION = '.tar.gz'\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "for file_name in FILES:\n",
    "    print('Opening', file_name, '...')\n",
    "    tar = tarfile.open(PREFIX + file_name + EXTENSION, \"r:gz\")\n",
    "    members = tar.getmembers()\n",
    "    member_count = len(members)\n",
    "    print('There are {} files in {}.'.format(member_count, file_name + EXTENSION))\n",
    "    for member_index in range(member_count):\n",
    "        print(member_index, 'out of', member_count, end='\\r', flush=True)\n",
    "        file_data = tar.extractfile(members[member_index])\n",
    "        if file_data is not None:\n",
    "            content = file_data.read()\n",
    "            data_rows.extend(read_protostream_file(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582070"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-453b86311259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression)\u001b[0m\n\u001b[1;32m   1415\u001b[0m                             \u001b[0mforce_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_ascii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_unit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                             \u001b[0mdefault_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1417\u001b[0;31m                             lines=lines, compression=compression)\n\u001b[0m\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(data_rows[:-1]).to_json('s.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Single `.protostream` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each .protostream file contains exactly 200 distinct records\n",
    "\n",
    "def read_protostream_entry(file):\n",
    "    decoder = _DecodeVarint32\n",
    "    \n",
    "    ## get the three types of protobuf messages we expect to see\n",
    "    header    = proto_stream.ProtoStreamHeader()\n",
    "    delimiter = proto_stream.ProtoStreamDelimiter()\n",
    "    \n",
    "    ## get the header\n",
    "    # length, pos = decoder(file, pos) => pos is starting point, length is the length of the record\n",
    "    length, pos = decoder(file, 0)\n",
    "    header.ParseFromString(file[pos:pos + length])\n",
    "    # print(header)\n",
    "    ## should check its contents\n",
    "\n",
    "    pos += length\n",
    "    data_rows = []\n",
    "\n",
    "    while pos < len(file):\n",
    "        length, pos = decoder(file, pos)\n",
    "        delimiter.ParseFromString(file[pos:pos + length])\n",
    "        if delimiter.delimiter_type == delimiter.END:\n",
    "            break\n",
    "        elif delimiter.delimiter_type == delimiter.ENTRY:\n",
    "            pos += length\n",
    "            length, pos = decoder(file, pos)\n",
    "            entry = proto_api.Entry()\n",
    "            entry.ParseFromString(file[pos:pos + length])\n",
    "            data_rows.append(entry)\n",
    "        pos += length\n",
    "    return data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting filtering Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the [Wikipedia page](https://en.wikipedia.org/wiki/Tunisian_Revolution) for the thematic we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from collections import Counter\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KeyWords(wiki_page, num_keywords, language, stop_words_language):\n",
    "    #download wikipedia page\n",
    "    wikipedia.set_lang(language)\n",
    "    wiki = wikipedia.page(wiki_page)\n",
    "\n",
    "    #initialize token's counter\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    wiki_tokens = [token for token in tokenizer.tokenize(wiki.content.lower()) \n",
    "                   if token not in set(stopwords.words(stop_words_language))]\n",
    "    count = Counter(wiki_tokens)\n",
    "    \n",
    "    #remove stop words\n",
    "    #sorted_count = sorted(count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #return [i[0] for i in sorted_count][:num_keywords]\n",
    "    return count.most_common(num_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sahara', 81),\n",
       " ('morocco', 78),\n",
       " ('western', 75),\n",
       " ('moroccan', 57),\n",
       " ('sahrawi', 53),\n",
       " ('territory', 46),\n",
       " ('polisario', 44),\n",
       " ('spanish', 39),\n",
       " ('area', 25),\n",
       " ('un', 24)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_KeyWords('Western Sahara', 10, 'en', 'english')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
