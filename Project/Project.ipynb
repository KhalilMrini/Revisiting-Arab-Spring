{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import codecs\n",
    "import zlib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spinn3rApi_pb2 as proto_api\n",
    "import protoStream_pb2 as proto_stream\n",
    "from google.protobuf.internal.decoder import _DecodeVarint32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each .protostream file contains exactly 200 distinct records\n",
    "\n",
    "def read_protostream_file(file):\n",
    "    decoder = _DecodeVarint32\n",
    "    \n",
    "    ## get the three types of protobuf messages we expect to see\n",
    "    header    = proto_stream.ProtoStreamHeader()\n",
    "    delimiter = proto_stream.ProtoStreamDelimiter()\n",
    "    \n",
    "    ## get the header\n",
    "    # length, pos = decoder(file, pos) => pos is starting point, length is the length of the record\n",
    "    length, pos = decoder(file, 0)\n",
    "    header.ParseFromString(file[pos:pos + length])\n",
    "    # print(header)\n",
    "    ## should check its contents\n",
    "\n",
    "    pos += length\n",
    "    data_rows = []\n",
    "\n",
    "    while pos < len(file):\n",
    "        length, pos = decoder(file, pos)\n",
    "        delimiter.ParseFromString(file[pos:pos + length])\n",
    "        if delimiter.delimiter_type == delimiter.END:\n",
    "            break\n",
    "        elif delimiter.delimiter_type == delimiter.ENTRY:\n",
    "            pos += length\n",
    "            length, pos = decoder(file, pos)\n",
    "            entry = proto_api.Entry()\n",
    "            entry.ParseFromString(file[pos:pos + length])\n",
    "            data = {}\n",
    "            \n",
    "            # Language\n",
    "            try:\n",
    "                data['Lang_Code'] = entry.feed_entry.lang[0].code\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Language Probability\n",
    "            try:\n",
    "                data['Lang_Prob'] = entry.feed_entry.lang[0].probability\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Title\n",
    "            try:\n",
    "                data['Post_Title'] = entry.feed_entry.title\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Content\n",
    "            try:\n",
    "                data['Post_Content'] = BeautifulSoup(zlib.decompress(\n",
    "                    entry.feed_entry.content.data), \"html.parser\").text\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Link\n",
    "            try:\n",
    "                data['Post_Link'] = entry.feed_entry.link[0].href\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Author Name\n",
    "            try:\n",
    "                data['Author_Name'] = entry.feed_entry.author[0].name\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Author Link\n",
    "            try:\n",
    "                data['Author_Link'] = entry.feed_entry.author[0].link[0].href\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Date & Time\n",
    "            try:\n",
    "                data['Datetime'] = entry.feed_entry.last_published\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Identifier\n",
    "            try:\n",
    "                data['Identifier'] = entry.feed_entry.identifier\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Spam Probability\n",
    "            try:\n",
    "                data['Spam'] = entry.feed_entry.spam_probability\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Publisher Type\n",
    "            try:\n",
    "                data['Type'] = entry.source.publisher_type\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Category\n",
    "            try:\n",
    "                data['Category'] = entry.feed_entry.category\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            data_rows.append(data)\n",
    "        pos += length\n",
    "    return data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 01-14-OTHER ...\n",
      "There are 3577 files in 01-14-OTHER.tar.gz.\n",
      "39 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.fanfiction.net/s/4326417/1/Sweet_Darling_Harry_Potter\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://losangeles.craigslist.org/lac/bik/2159262301.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.eventfilmmaker.com/showthread.php/108412-PSD-Experience?goto=newpost\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.eventfilmmaker.com/showthread.php/108413-New-Trailer-watch-now?goto=newpost\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://redinteriordesigns.blogspot.com/2011/01/interior-design-photos.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=n9uSy2OD1Bo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://aeoncomputadoras.com/componentes/ram/pc-escritorio/ddr2/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://l8.sphotos.l3.fbcdn.net/hphotos-l3-snc4/hs1132.snc4/149455_1549941802701_1659780300_1336236_1296806_n.jpg\n",
      "\n",
      "http://sphotos.ak.fbcdn.net/hphotos-ak-snc4/hs766.snc4/66604_445849826119_262902376119_6061706_7256736_n.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=H-s1MKMkl1k\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://pics.plentyoffish.com/dating/78/63/yguh4g45yu_116926093.jpg\n",
      "\n",
      "http://pics.plentyoffish.com/dating/78/54/2wyorrb2rj_116662594.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://l.yimg.com/eb/ymv/us/img/hv/photo/movie_pix/warner_brothers/racing_stripes/mandy_moore/racingstripes.jpg\n",
      "\n",
      "Thanks!!!\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=NK-_uNSOhww\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381 out of 3577 out of 3577\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khalil/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://hotair.com/archives/2011/01/12/palin-aide-shes-getting-death-threats-at-unprecedented-levels/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425 out of 3577\r"
     ]
    }
   ],
   "source": [
    "PREFIX = '/Volumes/Khalil Mrini/'\n",
    "FILES = '01-14-OTHER 01-14-SOCIAL_MEDIA 02-11-OTHER 02-11-SOCIAL_MEDIA'.split(' ')\n",
    "EXTENSION = '.tar.gz'\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "for file_name in FILES:\n",
    "    print('Opening', file_name, '...')\n",
    "    tar = tarfile.open(PREFIX + file_name + EXTENSION, \"r:gz\")\n",
    "    members = tar.getmembers()\n",
    "    member_count = len(members)\n",
    "    print('There are {} files in {}.'.format(member_count, file_name + EXTENSION))\n",
    "    for member_index in range(member_count):\n",
    "        print(member_index, 'out of', member_count, end='\\r', flush=True)\n",
    "        file_data = tar.extractfile(members[member_index])\n",
    "        if file_data is not None:\n",
    "            content = file_data.read()\n",
    "            data_rows.extend(read_protostream_file(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Single `.protostream` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each .protostream file contains exactly 200 distinct records\n",
    "\n",
    "def read_protostream_entry(file):\n",
    "    decoder = _DecodeVarint32\n",
    "    \n",
    "    ## get the three types of protobuf messages we expect to see\n",
    "    header    = proto_stream.ProtoStreamHeader()\n",
    "    delimiter = proto_stream.ProtoStreamDelimiter()\n",
    "    \n",
    "    ## get the header\n",
    "    # length, pos = decoder(file, pos) => pos is starting point, length is the length of the record\n",
    "    length, pos = decoder(file, 0)\n",
    "    header.ParseFromString(file[pos:pos + length])\n",
    "    # print(header)\n",
    "    ## should check its contents\n",
    "\n",
    "    pos += length\n",
    "    data_rows = []\n",
    "\n",
    "    while pos < len(file):\n",
    "        length, pos = decoder(file, pos)\n",
    "        delimiter.ParseFromString(file[pos:pos + length])\n",
    "        if delimiter.delimiter_type == delimiter.END:\n",
    "            break\n",
    "        elif delimiter.delimiter_type == delimiter.ENTRY:\n",
    "            pos += length\n",
    "            length, pos = decoder(file, pos)\n",
    "            entry = proto_api.Entry()\n",
    "            entry.ParseFromString(file[pos:pos + length])\n",
    "            data_rows.append(entry)\n",
    "        pos += length\n",
    "    return data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages: en, fr, ar\n",
    "Keywords:\n",
    "- Tunisia: tunis+\n",
    "- Egypt: egypt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
