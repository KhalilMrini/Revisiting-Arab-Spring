{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting filtering Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the [Wikipedia page](https://en.wikipedia.org/wiki/Tunisian_Revolution) for the thematic we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from collections import Counter\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "import string\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KeyWords(wiki_page, num_keywords, language, stop_words_language):\n",
    "    #download wikipedia page\n",
    "    wikipedia.set_lang(language)\n",
    "    wiki = wikipedia.page(wiki_page)\n",
    "\n",
    "    #initialize token's counter\n",
    "    tknzr = TweetTokenizer()\n",
    "    wiki_tokens = tknzr.tokenize(wiki.content)\n",
    "    count = Counter(wiki_tokens)  \n",
    "    \n",
    "    #remove stop words\n",
    "    count_filtered = {k.lower():v for k,v in count.items() if k.lower() not in set(stopwords.words(stop_words_language)) \n",
    "                      and k not in string.punctuation}\n",
    "    sorted_count = sorted(count_filtered.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return [i[0] for i in sorted_count][:num_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ben',\n",
       " 'tunisia',\n",
       " 'tunisian',\n",
       " 'ali',\n",
       " 'government',\n",
       " 'january',\n",
       " 'said',\n",
       " 'would',\n",
       " 'arab',\n",
       " 'february']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_KeyWords('Tunisian Revolution', 10, \"en\", \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2011',\n",
       " 'manifestações',\n",
       " 'tunísia',\n",
       " 'ben',\n",
       " 'ali',\n",
       " 'janeiro',\n",
       " 'anos',\n",
       " 'é',\n",
       " 'governo',\n",
       " 'país']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_KeyWords('Revolução de Jasmim', 10, \"pt\", \"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(wiki_page, num_keywords, language, stop_words_language, treshold):\n",
    "    #download wikipedia page\n",
    "    wikipedia.set_lang(language)\n",
    "    wiki = wikipedia.page(wiki_page)\n",
    "    \n",
    "    bigram_measures = BigramAssocMeasures\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(wiki.content)\n",
    "    tokens_filtered = [token for token in tokens if token.lower() not in set(stopwords.words(stop_words_language))]\n",
    "    \n",
    "    finder = BigramCollocationFinder.from_words(tokens_filtered)\n",
    "    finder.apply_freq_filter(treshold)\n",
    "    \n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    \n",
    "    return sorted(bigram for bigram, score in scored)  # doctest: +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('14', 'January'),\n",
       " ('Abidine', 'Ben'),\n",
       " ('Al', 'Jazeera'),\n",
       " ('Arab', 'world'),\n",
       " ('Ben', 'Ali'),\n",
       " ('December', '2010'),\n",
       " ('El', 'Abidine'),\n",
       " ('Fouad', 'Mebazaa'),\n",
       " ('January', '2011'),\n",
       " ('Jasmine', 'Revolution'),\n",
       " ('Mohamed', 'Bouazizi'),\n",
       " ('President', 'Zine'),\n",
       " ('Prime', 'Minister'),\n",
       " ('Saudi', 'Arabia'),\n",
       " ('Sidi', 'Bouzid'),\n",
       " ('United', 'States'),\n",
       " ('Zine', 'El'),\n",
       " ('civil', 'war'),\n",
       " ('days', 'later'),\n",
       " ('interim', 'government'),\n",
       " ('new', 'government'),\n",
       " ('security', 'forces'),\n",
       " ('state', 'emergency'),\n",
       " ('tear', 'gas')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams(\"Tunisian Revolution\", 30, \"en\", \"english\", treshold = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages: en, fr, ar\n",
    "Keywords:\n",
    "- Tunisia: tunis+\n",
    "- Egypt: egypt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
