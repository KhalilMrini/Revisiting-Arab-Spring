{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selecting Keyphrases to Filter Text Data\n",
    "\n",
    "**Problem**\n",
    "\n",
    "Having seen the data, we noticed that large parts of it were not related to the Arab Spring revolutions of 2011. Spinn3r, the company providing the data, is actually promoting their scraping API by doing so.\n",
    "\n",
    "Therefore, we must find keyphrases to filter this data based on the Tunisian and Egyptian revolutions.\n",
    "\n",
    "**Solution Proposed**\n",
    "\n",
    "Using **Wikipedia** articles for the Tunisian and Egyptian revolutions, we ***automatically*** extract the keyphrase that will be used for filtering. Given that the data is provided in various languages, we choose to focus on three relevant languages: **English**, **French**, and **Arabic**.\n",
    "\n",
    "**Added Value**\n",
    "\n",
    "In comparison with the only paper participating in the 2011 ICWSM Spinn3r Challenge, <a href='http://www.icwsm.org/2011/documents/IDC2011.pdf'>*Revolution 2.0 in Tunisia and Egypt: Reactions and Sentiments in the Online World*</a>:\n",
    "1. We use **keyphrases** instead of **keywords**, by using n-grams;\n",
    "2. We do not choose keyphrases manually, but extract them from Wikipedia automatically;\n",
    "3. We provide insight by evaluating data not only in English, but in two additional languages: French, Tunisia's colonial-era and second most popular language, and Arabic, the native language of both the peoples of Tunisia and Egypt.\n",
    "\n",
    "**In the following:** we first establish helper functions before compiling it in one function to get the keyphrases for each language, and each country.\n",
    "\n",
    "## 1.1. Tokenization and Filtering based on Parts of Speech\n",
    "\n",
    "The following are the languages that we focus on throughout the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN = 'english'\n",
    "FR = 'french'\n",
    "AR = 'arabic'\n",
    "LANGS = [EN, FR, AR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a *Regular Expression Tokenizer* with the *NLTK* (Natural Language Toolkit) package for the textual data.\n",
    "\n",
    "We want our keyphrases to contain **interesting words**, and therefore we eliminate stop words using the corresponding corpus from NLTK, and we eliminate verbs and adverbs using the **state-of-the-art Stanford POS Tagger**. We have to check the distinct *tag sets* of each language's POS tagger for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Imports\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "# Setting up POS Tagger and Stop Word Lists\n",
    "TOKENIZER = RegexpTokenizer(r'\\w+')\n",
    "STOPWORDS = dict((language, set(stopwords.words(language))) for language in LANGS)\n",
    "\n",
    "# Setting up POS Taggers\n",
    "JAR = './stanford-postagger.jar'\n",
    "POS_TAGGERS = dict((language, StanfordPOSTagger(language + '.tagger', JAR)) for language in LANGS)\n",
    "\n",
    "# Tags of Adverbs and Verbs in each language's POS Tagger\n",
    "REFUSED_POS_TAGS = {EN: ['V', 'R', 'MD', 'IN', 'CD'], \n",
    "                    FR: ['ADV', 'D', 'PREF', 'V'],\n",
    "                    AR: ['V']}\n",
    "\n",
    "def tag_is_allowed(tag, language):\n",
    "    \"\"\"\n",
    "    Determines if given tag is not a verb or adverb using REFUSED_POS_TAGS.\n",
    "    :param tag: string, tag of a word as determined by POS Tagger\n",
    "    :param language: string\n",
    "    :return: Boolean, True if tag should be kept\n",
    "    \"\"\"\n",
    "    for other_tag in REFUSED_POS_TAGS[language]:\n",
    "        if tag.startswith(other_tag):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def preprocess_for_keyphrases(text, language):\n",
    "    \"\"\"\n",
    "    Tokenizes, removes stopwords and digit-only tokens, removes verbs and adverbs.\n",
    "    :param text: string\n",
    "    :param language: string, language of text\n",
    "    :return: list of strings, tokens of text\n",
    "    \"\"\"\n",
    "    # Removing stop words, digit-only tokens\n",
    "    tokens = [token for token in TOKENIZER.tokenize(text.lower()) \n",
    "              if token not in STOPWORDS[language] \n",
    "              and not token.isdigit()]\n",
    "    # POS-Tagging\n",
    "    tokens_with_tags = POS_TAGGERS[language].tag(tokens)\n",
    "    # Handling the case of the Arabic POS Tagger, as it concatenates token and POS tag\n",
    "    if language == AR:\n",
    "        tokens_with_tags = [tag[1].split('/') for tag in tokens_with_tags]\n",
    "    # POS-Tag filtering\n",
    "    tokens = [token for token, tag in tokens_with_tags \n",
    "              if tag_is_allowed(tag, language)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Forming N-Grams\n",
    "\n",
    "Having formed tokens, we want now to give sense to keyphrases by forming n-grams. We use NLTK for that as well. We use the regular expression package to have the newly-formed n-grams as one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK and regexp Imports\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "import re\n",
    "\n",
    "def form_n_grams(tokens, threshold):\n",
    "    \"\"\"\n",
    "    Forms tokens that are n-grams with a minimum frequency threshold.\n",
    "    :param tokens: list of strings\n",
    "    :param threshold: int\n",
    "    :return: list of strings, n-gram'd tokens\n",
    "    \"\"\"\n",
    "    # Bigram Finder\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(threshold)\n",
    "    # Bigrams\n",
    "    scored_bigrams = finder.score_ngrams(BigramAssocMeasures.raw_freq)\n",
    "    # Joining Bigrams in the text\n",
    "    text = ' '.join(tokens)\n",
    "    for (word1, word2), score in scored_bigrams:\n",
    "        text = re.sub(r'\\b{}\\b \\b{}\\b'.format(word1, word2), '{}_{}'.format(word1, word2), text)\n",
    "    tokens = text.split(' ')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We form n-grams over different rounds, each round incrementing the *n* parameter.\n",
    "\n",
    "Given that Arabic is more agglutinative than English and French, it gets less rounds. The following are the rounds that we define to form n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_RANGE = {EN: range(3), \n",
    "               FR: range(3), \n",
    "               AR: range(1,3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function forms the n-grams over rounds. To keep interesting tokens, we remove tokens that have less than 4 characters. We remove these tokens only after forming n-grams because we noticed that short unigrams can form interesting n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHAR_COUNT = 4\n",
    "\n",
    "def n_gram_rounds(tokens, language, threshold):\n",
    "    \"\"\"\n",
    "    Forms n-grams over rounds given unigram tokens.\n",
    "    :param tokens: list of strings\n",
    "    :param language: string\n",
    "    :param threshold: int\n",
    "    :return: list of strings, n-gram tokens\n",
    "    \"\"\"\n",
    "    for ngram_round in NGRAM_RANGE[language]:\n",
    "        tokens = form_n_grams(tokens, threshold + ngram_round)\n",
    "    tokens = [token for token in tokens if len(token) >= MIN_CHAR_COUNT]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Downloading Wikipedia Articles\n",
    "\n",
    "Throughout the project, we focus on the two following countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EG = 'Egypt'\n",
    "TN = 'Tunisia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following articles to extract keyphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_ARTICLES = {TN: {EN: 'Tunisian Revolution', \n",
    "                      FR: 'Révolution tunisienne', \n",
    "                      AR: 'الثورة التونسية'},\n",
    "                 EG: {EN: 'Egyptian Revolution', \n",
    "                      FR: 'Révolution égyptienne de 2011', \n",
    "                      AR: 'ثورة 25 يناير'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *Wikipedia* Python package to extract article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia import\n",
    "import wikipedia\n",
    "\n",
    "def download_wikipedia_article(wiki_article, language):\n",
    "    \"\"\"\n",
    "    Downloads the text content of a given Wikipedia article in a given language.\n",
    "    :param wiki_article: string, title of article\n",
    "    :param language: string\n",
    "    :return: string\n",
    "    \"\"\"\n",
    "    wikipedia.set_lang(language[:2])\n",
    "    wiki = wikipedia.page(wiki_article)\n",
    "    return wiki.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Extracting Most Popular Tokens\n",
    "\n",
    "Once we have formed phrases and have them as a list of strings, we use `Counter` to get the most popular phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_most_popular_tokens(tokens, num_keyphrases):\n",
    "    \"\"\"\n",
    "    Gets most popular num_keywords elements of tokens.\n",
    "    :param tokens: list of strings\n",
    "    :param num_keyphrases: int\n",
    "    :return: list of strings\n",
    "    \"\"\"\n",
    "    counter = Counter(tokens)\n",
    "    keyphrases = [word.replace('_', ' ') for word, score in counter.most_common(num_keyphrases)]\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Extracting Keyphrases from Wikipedia Articles\n",
    "\n",
    "We concatenate all of the above in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_keyphrases(wiki_article, language, num_keyphrases=50, threshold=1):\n",
    "    \"\"\"\n",
    "    Gets Keyphrases from Wikipedia Article.\n",
    "    :param wiki_article: string, name of Wikipedia Article\n",
    "    :param language: string\n",
    "    :param num_keyphrases: int, number of keyphrases to extract\n",
    "    :param threshold: int, n-gram formation threshold\n",
    "    :return: list of strings, keyphrases\n",
    "    \"\"\"\n",
    "    wiki_content = download_wikipedia_article(wiki_article, language)\n",
    "    unigram_tokens = preprocess_for_keyphrases(wiki_content, language)\n",
    "    ngram_tokens = n_gram_rounds(unigram_tokens, language, threshold)\n",
    "    keyphrases = get_most_popular_tokens(ngram_tokens, num_keyphrases)\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract 50 keyphrases, per country, and per language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase_dict = dict((country, \n",
    "                       dict((language, get_wikipedia_keyphrases(WIKI_ARTICLES[country][language], language)) \n",
    "                            for language in WIKI_ARTICLES[country])) \n",
    "                      for country in WIKI_ARTICLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Examples of Keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we provide an example of keyphrases for English in **Tunisia**. The most popular is `ben ali`, the last name of Tunisia's ousted president. The phrase `sidi bouzid` designates the city where protests started in January 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ben ali',\n",
       " 'tunisia',\n",
       " 'arab world',\n",
       " 'protests',\n",
       " 'prime minister',\n",
       " 'sidi bouzid',\n",
       " 'tunisian',\n",
       " 'security forces',\n",
       " 'foreign minister',\n",
       " 'january']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrase_dict[TN][EN][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereafter is an sample of keyphrases for English in **Egypt**. The most popular one is `tahrir square`, a public square in the Egyptian capital, Cairo, where the protesters camped throughout the protests. The `muslim brotherhood` is the opposition party that later came into power. Here, the Egyptian ousted president's full name, `hosni mubarak`, comes up as one of the keyphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tahrir square',\n",
       " 'muslim brotherhood',\n",
       " 'revolution',\n",
       " 'egyptian revolution',\n",
       " 'hosni mubarak',\n",
       " 'social media',\n",
       " 'protesters',\n",
       " 'prime minister',\n",
       " 'protests',\n",
       " 'human rights']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrase_dict[EG][EN][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Comparing Texts to Keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these keyphrases, we want to estimate how many of those appear in a given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyphrase_score(country, language, text):\n",
    "    \"\"\"\n",
    "    Computes how many keyphrases for the country and language are in the text.\n",
    "    :param country: string\n",
    "    :param language: string\n",
    "    :param text: string\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for keyphrase in keyphrase_dict[country][language]:\n",
    "        if keyphrase in filtered_text:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to compute the keyphrase scores per country and for a given language and a given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keyphrase_scores(text, language):\n",
    "    \"\"\"\n",
    "    Computes keyphrase scores for all countries given text and language.\n",
    "    Returns False if all scores are zero.\n",
    "    :param text: string\n",
    "    :param language: string\n",
    "    :return: dict or boolean\n",
    "    \"\"\"\n",
    "    filtered_text = ' '.join(preprocess_for_keyphrases(text, language))\n",
    "    data = {}\n",
    "    score_sum = 0\n",
    "    for country in keyphrase_dict:\n",
    "        score = keyphrase_score(country, language, filtered_text)\n",
    "        data[country] = score\n",
    "        score_sum += score\n",
    "    if score_sum == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sampling and Filtering the 2.1 TB of Compressed Data\n",
    "\n",
    "**Steps to Reach Text Data**\n",
    "\n",
    "The files are compressed in `.tar.gz` format. The steps to reach text data are more complex than we thought:\n",
    "1. We downloaded a sample, and the decompressed files are in `.protostream` format. This format **lacks documentation**. A `.protostream` file is a stream of Google's `protobuf` format.\n",
    "2. The `.protostream` format requires `.proto` files to extract them. We found these files on the internet, not on Spinn3r's website. This format was originally made for Java, and has difficult compatibility with Python.\n",
    "3. We installed the C++ `protoc` library to make `.py` files out of `.proto` files. These `.py` files contain Python classes that we can use to read the data. The `.proto` and `.py` files can be found on our GitHub repo.\n",
    "4. We found the code to read the text data on a *Stackoverflow* answer rated `-1`, rather than on Spinn3r's website or the `protobuf` documentation.\n",
    "\n",
    "**Sample Evaluated**\n",
    "\n",
    "We downloaded compressed samples of about **40 GB** for the days of **January 14, 2011** and **February 11, 2011**, respectively the days of the resignation of the presidents of Tunisia and Egypt.\n",
    "\n",
    "**In the following:** we first read the data and filter it based on the keywords found in the first section.\n",
    "\n",
    "## 2.1. Reading a Single `.protostream` File\n",
    "\n",
    "We first import the `.py` files that we got out of the `.proto` files. The `spinn3rApi_pb2.py` file contains the classes needed to parse the bytes of a given internet entry. The `protoStream_pb2.py` file contains the classes needed to separate entries inside a `.protostream` file, most notably the `ProtoStreamDelimiter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinn3rApi_pb2 as proto_api\n",
    "import protoStream_pb2 as proto_stream\n",
    "\n",
    "DELIMITER = proto_stream.ProtoStreamDelimiter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following decoder to separate entries in a `.protostream` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf.internal.decoder import _DecodeVarint32\n",
    "\n",
    "# length, pos = decoder(file, pos) => pos is starting point, length is the length of the record in file\n",
    "DECODER = _DecodeVarint32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the files we find, we find the languages as two-letter language codes. We will need a mapping to full-length name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_TO_LANG = dict((language[:2], language) for language in LANGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following functions to read a single `.protostream` file onto a list of `dict` instances. We use `BeautifulSoup` for some `html` data, that is itself `zlib`-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def try_to_get_content(entry):\n",
    "    \"\"\"\n",
    "    Attempts to get content from an entry.\n",
    "    :param entry: contentApi.Entry\n",
    "    :return: string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return BeautifulSoup(zlib.decompress(entry.feed_entry.content.data), \"html.parser\").text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def read_protostream_file(file):\n",
    "    \"\"\"\n",
    "    Reads a single .protostream file.\n",
    "    :param file: bytes\n",
    "    :return: list of dictionaries\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    length, pos = decoder(file, 0)\n",
    "    pos += length\n",
    "\n",
    "    while pos < len(file):\n",
    "        length, pos = decoder(file, pos)\n",
    "        DELIMITER.ParseFromString(file[pos:pos + length])\n",
    "        if DELIMITER.delimiter_type == DELIMITER.END:\n",
    "            break\n",
    "            \n",
    "        # This means we found an Entry\n",
    "        elif DELIMITER.delimiter_type == DELIMITER.ENTRY:\n",
    "            pos += length\n",
    "            length, pos = decoder(file, pos)\n",
    "            \n",
    "            # The Entry Class enables to read the contents of the record\n",
    "            entry = proto_api.Entry()\n",
    "            entry.ParseFromString(file[pos:pos + length])\n",
    "            \n",
    "            lang_code = entry.feed_entry.lang[0].code\n",
    "            \n",
    "            # Proceeding if the language is English, French or Arabic\n",
    "            if lang_code in CODE_TO_LANG:\n",
    "                post_title = entry.feed_entry.title\n",
    "                post_content = try_to_get_content(entry)\n",
    "                data = compute_keyphrase_scores(post_title + ' ' + post_content, \n",
    "                                                CODE_TO_LANG[lang_code])\n",
    "                if data:\n",
    "                    # Language Code\n",
    "                    data['Lang_Code'] = lang_code\n",
    "                    \n",
    "                    # Post Title\n",
    "                    data['Post_Title'] = post_title\n",
    "                    \n",
    "                    # Post Content\n",
    "                    if post_content:\n",
    "                        data['Post_Content'] = post_content\n",
    "                    \n",
    "                    # Language Probability\n",
    "                    try:\n",
    "                        data['Lang_Prob'] = entry.feed_entry.lang[0].probability\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Link\n",
    "                    try:\n",
    "                        data['Post_Link'] = entry.feed_entry.link[0].href\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Author Name\n",
    "                    try:\n",
    "                        data['Author_Name'] = entry.feed_entry.author[0].name\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Author Link\n",
    "                    try:\n",
    "                        data['Author_Link'] = entry.feed_entry.author[0].link[0].href\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Date & Time\n",
    "                    try:\n",
    "                        data['Datetime'] = entry.feed_entry.last_published\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Identifier\n",
    "                    try:\n",
    "                        data['Identifier'] = entry.feed_entry.identifier\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Spam Probability\n",
    "                    try:\n",
    "                        data['Spam'] = entry.feed_entry.spam_probability\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Publisher Type\n",
    "                    try:\n",
    "                        data['Type'] = entry.source.publisher_type\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Category\n",
    "                    try:\n",
    "                        data['Category'] = entry.feed_entry.category\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    data_rows.append(data)\n",
    "        pos += length\n",
    "    return data_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Reading the Compressed `.tar.gz` Files\n",
    "\n",
    "We downloaded the files in an external harddrive. We set up the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = '/Volumes/Khalil Mrini/'\n",
    "FILES = '01-14-OTHER 01-14-SOCIAL_MEDIA 02-11-OTHER 02-11-SOCIAL_MEDIA'.split(' ')\n",
    "EXTENSION = '.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a `.tar.gz` file without decompressing it, we use the following package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we read the `.tar.gz` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 01-14-OTHER ...\n"
     ]
    }
   ],
   "source": [
    "data_rows = []\n",
    "\n",
    "for file_name in FILES:\n",
    "    print('Opening', file_name, '...')\n",
    "    tar = tarfile.open(PREFIX + file_name + EXTENSION, \"r:gz\")\n",
    "    members = tar.getmembers()\n",
    "    member_count = len(members)\n",
    "    print('There are {} files in {}.'.format(member_count, file_name + EXTENSION))\n",
    "    for member_index in range(member_count):\n",
    "        print(member_index, 'out of', member_count, end='\\r', flush=True)\n",
    "        file_data = tar.extractfile(members[member_index])\n",
    "        if file_data is not None:\n",
    "            content = file_data.read()\n",
    "            data_rows.extend(read_protostream_file(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Putting the Collected Data in a `DataFrame`\n",
    "\n",
    "After having collected and filtered the data, we put it in a `Pandas` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_rows)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
